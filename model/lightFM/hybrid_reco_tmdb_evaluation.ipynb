{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usr1jvQgf84U"
      },
      "outputs": [],
      "source": [
        "# !pip install lightfm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHn770FH1E5"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVRZUVEw7KzK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "MOVIE_METADATA_PATH = \"movie_enriched_view.csv\"\n",
        "USER_RATINGS_PATH = \"user_ratings_200users_30each.csv\"\n",
        "movies_df = pd.read_csv(MOVIE_METADATA_PATH)\n",
        "users_df = pd.read_csv(USER_RATINGS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L64CEFWMcpwA"
      },
      "source": [
        "##NOTE Generate numeric identifier: LightFM python only except numeric id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNgk0i8uHykO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_int_id(df: pd.DataFrame, col_name: str, new_col_name: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a string/categorical column into unique integer IDs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input DataFrame.\n",
        "    col_name : str\n",
        "        Name of the column to encode (e.g. 'user_id' or 'movie_id').\n",
        "    new_col_name : str, optional\n",
        "        Name of the new column to store integer IDs.\n",
        "        - If provided: a new column is created.\n",
        "        - If None: the original column is overwritten.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with the encoded integer column.\n",
        "    \"\"\"\n",
        "\n",
        "    # Factorize the column: assigns a unique integer to each unique value\n",
        "    # e.g. ['u1', 'u2', 'u1'] → [0, 1, 0]\n",
        "    codes, uniques = pd.factorize(df[col_name])\n",
        "\n",
        "    # Determine where to store the result:\n",
        "    # - Use new_col_name if provided\n",
        "    # - Otherwise overwrite the original column\n",
        "    target_col = new_col_name if new_col_name is not None else col_name\n",
        "\n",
        "    # Store the encoded values in the target column\n",
        "    df[target_col] = codes.astype('int64')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND1MK95WZBoE"
      },
      "outputs": [],
      "source": [
        "users_df = generate_int_id(users_df, \"user_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2eKO6oZdwpq"
      },
      "source": [
        "# Buildinf the ID mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hiiBzveFNk8"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hVk6fZElKNk"
      },
      "source": [
        "Note that if we don’t have all user and items ids at once, we can repeatedly call fit_partial to supply additional ids. In this case, we will use this capability to add some item feature mappings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qDR1-zMbf3M"
      },
      "source": [
        "# Item Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JRD6hlJO8z9"
      },
      "outputs": [],
      "source": [
        "def extract_item_feature_list(df, feature_columns):\n",
        "    \"\"\"\n",
        "    Extract unique lowercase feature values from selected metadata columns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame containing movie metadata.\n",
        "    feature_columns : list of str\n",
        "        Column names to extract features from.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Set[str]\n",
        "        Set of all unique feature names (lowercased).\n",
        "    \"\"\"\n",
        "    movie_all_features = set()\n",
        "    for _, row in df.iterrows():\n",
        "        combined = []\n",
        "        for col in feature_columns:\n",
        "            combined.extend(str(row[col]).split(','))\n",
        "        cleaned = [f.strip().lower() for f in combined if f.strip()]\n",
        "        movie_all_features.update(cleaned)\n",
        "    return  list(movie_all_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGmK12hCO9pw"
      },
      "outputs": [],
      "source": [
        "def build_item_feature_tuples(df, feature_columns):\n",
        "    \"\"\"\n",
        "    Build (movie_id, [features...]) tuples for LightFM item_features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame containing movie metadata.\n",
        "    feature_columns : list of str\n",
        "        Columns to use for feature extraction.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Tuple[int, List[str]]]\n",
        "        Tuples of movie_id and its feature list.\n",
        "    \"\"\"\n",
        "    tuples = []\n",
        "    for _, row in df.iterrows():\n",
        "        combined = []\n",
        "        for col in feature_columns:\n",
        "            combined.extend(str(row[col]).split(','))\n",
        "        cleaned = [f.strip().lower() for f in combined if f.strip()]\n",
        "        tuples.append((row['movie_id'], cleaned))\n",
        "    return tuples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvmnWm9ARTCq"
      },
      "outputs": [],
      "source": [
        "from lightfm.data import Dataset\n",
        "\n",
        "# Step 1: Prepare dataset\n",
        "dataset = Dataset()\n",
        "dataset.fit(users_df['user_id'], movies_df['movie_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiG4j0BkfnGc"
      },
      "outputs": [],
      "source": [
        "### TODO Rewrite Function\n",
        "# Step 2: Extract feature set and fit dataset\n",
        "feature_columns = ['genres', 'keywords']\n",
        "movie_all_features = extract_item_feature_list(movies_df, feature_columns)\n",
        "dataset.fit_partial(items=movies_df['movie_id'], item_features=movie_all_features)\n",
        "\n",
        "# TODO check build_item_features parameter format\n",
        "# Step 3: Build item_features matrix\n",
        "item_feature_tuples = build_item_feature_tuples(movies_df, feature_columns)\n",
        "item_features = dataset.build_item_features(item_feature_tuples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maco Variable"
      ],
      "metadata": {
        "id": "xD2O6IEGdvC9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wlk4JqQRn3mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METHOD_STR=\"random_train_test_split\" # split_user_interactions_df random_train_test_split\n",
        "WEIGHT_METHOD_STR = \"default\" # default , condition_3, ratings\n"
      ],
      "metadata": {
        "id": "11nxpLxZdzl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def split_user_interactions_df(df, user_col='user_id', test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Optimized version: split each user's interactions into train and test sets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with user-item interactions.\n",
        "    user_col : str\n",
        "        Column for user ID.\n",
        "    test_size : float\n",
        "        Fraction of each user's data to put into test.\n",
        "    random_state : int\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_df : pd.DataFrame\n",
        "    test_df : pd.DataFrame\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Assign a row number within each user group\n",
        "    df = df.copy()\n",
        "    df['_row_id'] = df.groupby(user_col).cumcount()\n",
        "\n",
        "    # Count interactions per user\n",
        "    user_counts = df.groupby(user_col)['_row_id'].max() + 1\n",
        "\n",
        "    test_rows = []\n",
        "    for user_id, count in user_counts.items():\n",
        "        if count < 2:\n",
        "            continue\n",
        "        test_size_u = max(1, int(count * test_size))\n",
        "        test_indices = rng.choice(count, size=test_size_u, replace=False)\n",
        "        test_rows.extend(df[(df[user_col] == user_id) & (df['_row_id'].isin(test_indices))].index)\n",
        "\n",
        "    test_df = df.loc[test_rows].drop(columns=['_row_id'])\n",
        "    train_df = df.drop(index=test_rows).drop(columns=['_row_id'])\n",
        "\n",
        "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "1vsHpUQTO7J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightfm.cross_validation import random_train_test_split\n",
        "\n",
        "def train_test_split_method_return_interaction(\n",
        "    method_str=None,\n",
        "    weight_method_str=None,\n",
        "    df=None,\n",
        "    user_col=\"user_id\",\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    dataset=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Split interactions and weights using selected strategy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    method_str : str\n",
        "        Splitting strategy, one of [\"split_user_interactions_df\", \"random_train_test_split\"].\n",
        "    weight_method_str : str\n",
        "        Weighting strategy, one of [\"default\", \"ratings\", \"condition_<rating_threshold>\"].\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with user_id, movie_id, rating.\n",
        "    user_col : str\n",
        "        Column for user IDs (default \"user_id\").\n",
        "    test_size : float\n",
        "        Proportion for test split.\n",
        "    random_state : int\n",
        "        Random seed.\n",
        "    dataset : lightfm.data.Dataset\n",
        "        LightFM dataset object used for building interactions.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    method_str, train_interactions, test_interactions, train_weights, test_weights\n",
        "    \"\"\"\n",
        "\n",
        "    def build_interactions_by_weight(df, strategy, dataset):\n",
        "        if strategy == \"default\":\n",
        "            return dataset.build_interactions([\n",
        "                (u, i, 1.0)\n",
        "                for u, i, r in zip(df[\"user_id\"], df[\"movie_id\"], df[\"rating\"])\n",
        "            ])\n",
        "        elif strategy == \"ratings\":\n",
        "            return dataset.build_interactions([\n",
        "                (u, i, r)\n",
        "                for u, i, r in zip(df[\"user_id\"], df[\"movie_id\"], df[\"rating\"])\n",
        "            ])\n",
        "        elif str(strategy).startswith(\"condition_\"):\n",
        "            threshold = int(strategy.split(\"_\")[1])\n",
        "            return dataset.build_interactions([\n",
        "                (u, i, 1.0 if r >= threshold else 0.0)\n",
        "                for u, i, r in zip(df[\"user_id\"], df[\"movie_id\"], df[\"rating\"])\n",
        "            ])\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid weight_method_str: {strategy}\")\n",
        "\n",
        "    if method_str == \"split_user_interactions_df\":\n",
        "        train_df, test_df = split_user_interactions_df(\n",
        "            df=df, user_col=user_col, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        train_interactions, train_weights = build_interactions_by_weight(train_df, weight_method_str, dataset)\n",
        "        test_interactions, test_weights = build_interactions_by_weight(test_df, weight_method_str, dataset)\n",
        "\n",
        "    elif method_str == \"random_train_test_split\":\n",
        "        full_interactions, full_weights = build_interactions_by_weight(df, weight_method_str, dataset)\n",
        "\n",
        "        train_interactions, test_interactions = random_train_test_split(\n",
        "            full_interactions, test_percentage=test_size, random_state=random_state\n",
        "        )\n",
        "        train_weights, test_weights = random_train_test_split(\n",
        "            full_weights, test_percentage=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid method_str: {method_str}\")\n",
        "\n",
        "    return method_str, train_interactions, test_interactions, train_weights, test_weights\n"
      ],
      "metadata": {
        "id": "HR_64h1eRWZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "method_str, train_interactions, test_interactions, train_weights, test_weights = \\\n",
        "    train_test_split_method_return_interaction(\n",
        "        method_str=METHOD_STR, # \"split_user_interactions_df\" \"random_train_test_split\"\n",
        "        weight_method_str=WEIGHT_METHOD_STR, #default, ratings, condition_3\n",
        "        df=users_df,\n",
        "        user_col=\"user_id\",\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        dataset=dataset\n",
        "    )"
      ],
      "metadata": {
        "id": "0S9M7lQHTR58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 10"
      ],
      "metadata": {
        "id": "_7j-khdNeHHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter\n"
      ],
      "metadata": {
        "id": "LSvylILVeSE-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6ar8Nblf_qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOSS_FUNCTION = \"warp\" # warp , bpr, logistic\n",
        "EPOCHS = 100\n",
        "L_RATE = 0.05\n",
        "NO_COM = 30\n",
        "MAX_SAM = 30\n"
      ],
      "metadata": {
        "id": "Tb2W48fmeWIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHLdH6xir0o1",
        "outputId": "9c57dd94-0905-4ea1-ad2d-79ca3eec4ef1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightfm.lightfm.LightFM at 0x790bd4c3bbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "from lightfm import LightFM\n",
        "\n",
        "# 3. Train the model using only the training interactions and corresponding weights\n",
        "model = LightFM(\n",
        "    loss=LOSS_FUNCTION, # warp , bpr, logistic\n",
        "    learning_rate=L_RATE,\n",
        "    no_components=NO_COM,\n",
        "    max_sampled=MAX_SAM,\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    interactions=train_interactions,\n",
        "    item_features=item_features,\n",
        "    sample_weight=train_weights,\n",
        "    epochs=EPOCHS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ydu1ALhQz2l"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Evaluation\"\"\"\n",
        "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
        "\n",
        "def evaluate_model(model, interactions, k_list, item_features=None, train_interactions=None, split_name=\"TRAIN\"):\n",
        "    precision_scores = {}\n",
        "    recall_scores = {}\n",
        "\n",
        "    for k in k_list:\n",
        "        precision = precision_at_k(\n",
        "            model, interactions, train_interactions=train_interactions,\n",
        "            item_features=item_features, k=k\n",
        "        ).mean()\n",
        "        recall = recall_at_k(\n",
        "            model, interactions, train_interactions=train_interactions,\n",
        "            item_features=item_features, k=k\n",
        "        ).mean()\n",
        "\n",
        "        precision_scores[k] = precision\n",
        "        recall_scores[k] = recall\n",
        "\n",
        "    auc = auc_score(\n",
        "        model, interactions, train_interactions=train_interactions,\n",
        "        item_features=item_features\n",
        "    ).mean()\n",
        "\n",
        "    # Print section\n",
        "    print(f\"{split_name} Evaluation\")\n",
        "    for k in k_list:\n",
        "        print(f\"Precision@{k}: {precision_scores[k]:.4f}\")\n",
        "        print(f\"Recall@{k}: {recall_scores[k]:.4f}\\n\")\n",
        "    print(f\"AUC: {auc:.4f}\\n\")\n",
        "\n",
        "    return precision_scores, recall_scores, auc\n"
      ],
      "metadata": {
        "id": "FDArn9eY_isT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "wcNN74_tUJc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set k values\n",
        "k_values = [3, 5, 10]\n",
        "\n",
        "\"\"\"# Summary\"\"\"\n",
        "print(\"Summary\")\n",
        "print(f\"Train Test Method: {METHOD_STR}\")\n",
        "print(f\"Weight Method: {WEIGHT_METHOD_STR}\")\n",
        "print(f\"Loss Function: {LOSS_FUNCTION}\")\n",
        "print(\"Epochs:\", EPOCHS)\n",
        "print(\"Learning rate:\", L_RATE)\n",
        "print(\"No. components:\", NO_COM)\n",
        "print(\"Max sampled:\", MAX_SAM)\n",
        "print()\n",
        "\n",
        "# Evaluate on train\n",
        "train_precision_scores, train_recall_scores, train_auc = evaluate_model(\n",
        "    model, train_interactions, k_values, item_features=item_features, split_name=\"TRAIN\"\n",
        ")\n",
        "\n",
        "# Evaluate on test\n",
        "test_precision_scores, test_recall_scores, test_auc = evaluate_model(\n",
        "    model, test_interactions, k_values, item_features=item_features,\n",
        "    train_interactions=train_interactions, split_name=\"TEST\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqwhJNcK_iV8",
        "outputId": "6cc4cf7b-152c-4857-8768-be1bc3785e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary\n",
            "Train Test Method: random_train_test_split\n",
            "Weight Method: default\n",
            "Loss Function: warp\n",
            "Epochs: 100\n",
            "Learning rate: 0.05\n",
            "No. components: 30\n",
            "Max sampled: 30\n",
            "\n",
            "TRAIN Evaluation\n",
            "Precision@3: 0.6417\n",
            "Recall@3: 0.0809\n",
            "\n",
            "Precision@5: 0.6170\n",
            "Recall@5: 0.1294\n",
            "\n",
            "Precision@10: 0.5970\n",
            "Recall@10: 0.2498\n",
            "\n",
            "AUC: 0.9999\n",
            "\n",
            "TEST Evaluation\n",
            "Precision@3: 0.2483\n",
            "Recall@3: 0.1186\n",
            "\n",
            "Precision@5: 0.2130\n",
            "Recall@5: 0.1744\n",
            "\n",
            "Precision@10: 0.1655\n",
            "Recall@10: 0.2806\n",
            "\n",
            "AUC: 0.9715\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}