{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d018d4f0",
   "metadata": {},
   "source": [
    "# Movie Recommendation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de41d3e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77125708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ast import literal_eval # For safely converting stringified lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8cc94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>353021</td>\n",
       "      <td>The Lankworm</td>\n",
       "      <td>drama</td>\n",
       "      <td>womandirector, condom</td>\n",
       "      <td>Film about a couple of which the man is confro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325023</td>\n",
       "      <td>The Datcha</td>\n",
       "      <td>adventure</td>\n",
       "      <td>womandirector</td>\n",
       "      <td>Jordy is an eleven year old boy who's family h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id         title     genres               keywords  \\\n",
       "0    353021  The Lankworm      drama  womandirector, condom   \n",
       "1    325023    The Datcha  adventure          womandirector   \n",
       "\n",
       "                                            overview  \n",
       "0  Film about a couple of which the man is confro...  \n",
       "1  Jordy is an eleven year old boy who's family h...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH  = \"D:/Github_ThisPC/TMDB_RecoFlow/airflow/data/cbf_movie.csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41d92a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 135886 entries, 0 to 135885\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   movie_id  135886 non-null  int64 \n",
      " 1   title     135886 non-null  object\n",
      " 2   genres    135886 non-null  object\n",
      " 3   keywords  135886 non-null  object\n",
      " 4   overview  135886 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40fd09c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135886, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812822c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['movie_id', 'title', 'genres', 'keywords', 'overview'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d5134",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bfe5069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_id    0\n",
       "title       0\n",
       "genres      0\n",
       "keywords    0\n",
       "overview    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4069eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44f51c",
   "metadata": {},
   "source": [
    "Download necessary NLTK data\n",
    "The code ensures that all required resources from the NLTK library are available for text processing. This is often necessary when working with stopwords or lemmatization. The resources are downloaded only if they are not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "391362bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data (only need to do this once)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize('test')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Download WordNet if not already available\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fceafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "stop_words = set(stopwords.words('english'))  # Define stop words for text processing\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer for stemming words to their root form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52114eb5",
   "metadata": {},
   "source": [
    "Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d93d3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from a given text.\"\"\"\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        return \" \".join(words)  # Join words back into a string\n",
    "    return \"\"\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize the words in a given text.\"\"\"\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "        return \" \".join(words)  # Join lemmatized words back into a string\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb87d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1.intersection(set2))  # Size of intersection\n",
    "    union = len(set1.union(set2))  # Size of union\n",
    "    return intersection / union if union != 0 else 0  # Avoid division by zero "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54252ba2",
   "metadata": {},
   "source": [
    "The safe_literal_eval function safely evaluates strings that may represent data structures like lists, tuples, or dictionaries. If the value is a string, it attempts to parse it using literal_eval, and if that fails, it splits the string by commas into a list. If the value is None or NaN, it returns an empty list. For other types of input, the function simply returns the value as-is. The function is applied to the keywords and genres columns of a DataFrame, ensuring that string representations of lists or similar structures are properly converted into Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a1480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    \"\"\"Safely evaluate a string to its literal representation.\"\"\"\n",
    "    if isinstance(value, str):  # Check if it's a string\n",
    "        try:\n",
    "            # Try to evaluate the string as a literal (e.g., list, tuple, dict)\n",
    "            return literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If evaluation fails, return the string as a list by splitting on commas\n",
    "            return value.split(',') if value else []\n",
    "    elif isinstance(value, float) and np.isnan(value):  # Handle NaN values\n",
    "        return []  # Return an empty list for NaN values\n",
    "    elif value is None:  # Handle None explicitly\n",
    "        return []  # Return an empty list for None\n",
    "    else:\n",
    "        return value  # Return the value as-is if it's already in the correct format\n",
    "\n",
    "# Apply the function to the specified columns\n",
    "features = ['keywords', 'genres']\n",
    "for feature in features:\n",
    "    df[feature] = df[feature].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "559c9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to convert all strings to lower case and strip names of spaces\n",
    "# def clean_data(x):\n",
    "#     if isinstance(x, list):\n",
    "#         return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "#     else:\n",
    "#         return ''\n",
    "    \n",
    "# # Apply clean_data function to your features.\n",
    "# features = ['keywords', 'genres']\n",
    "\n",
    "# for feature in features:\n",
    "#     df[feature] = df[feature].apply(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba74c0",
   "metadata": {},
   "source": [
    "1. Replaced cosine_similarity(...) with NearestNeighbors\n",
    "\n",
    "    cosine_similarity(tfidf_overview_matrix) \\\n",
    "    This computes a full pairwise similarity matrix for all movies. \\\n",
    "    If you had 60,000 movies, that’s 60,000 × 60,000 = 3.6 billion float values → ~29.1 GB in memory → 💥 MemoryError \n",
    "\n",
    "    Instead of calculating similarity between every pair, it only finds: \\\n",
    "    The K most similar movies for a given movie.\n",
    "\n",
    "2. Introduced get_similarity_from_knn(...)\n",
    "    Since we no longer have access to [i, j] entries in a cosine matrix, I added this helper:\n",
    "    ```\n",
    "    def get_similarity_from_knn(model, matrix, idx1, idx2):\n",
    "    distances, indices = model.kneighbors(matrix[idx1], n_neighbors=50)\n",
    "    if idx2 in indices[0]:\n",
    "        position = list(indices[0]).index(idx2)\n",
    "        return 1 - distances[0][position]\n",
    "    return 0.0\n",
    "    ```\n",
    "\n",
    "    This:\n",
    "    Finds top-50 most similar movies to idx1 \\\n",
    "    Checks if idx2 is among them \\\n",
    "    Returns the similarity value (1 - cosine distance) \\\n",
    "    ✅ This lets you still compute pairwise similarity on demand, but only for the movies you're actually comparing (not the full matrix).\n",
    "\n",
    "3. Updated combined_similarity() to Use This Helper\n",
    "    Instead of accessing: \\\n",
    "    cosine_sim_overview[i, j]\n",
    "\n",
    "    I now use: \\\n",
    "    overview_sim = get_similarity_from_knn(overview_nn, tfidf_overview_matrix, i, j) \\\n",
    "    So your combined scoring logic still works the same — it just pulls similarities differently now.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cd79a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you already have 'df' as your DataFrame\n",
    "# # Handle missing values in overview and keywords columns\n",
    "# df['overview'] = df['overview'].fillna('')  # Replace NaN in overview\n",
    "# df['keywords'] = df['keywords'].fillna('')  # Replace NaN in keywords\n",
    "\n",
    "# # Feature Extraction\n",
    "# # Create TF-IDF matrix for movie overviews\n",
    "# tfidf_overview = TfidfVectorizer(stop_words='english')\n",
    "# tfidf_overview_matrix = tfidf_overview.fit_transform(df['overview'])\n",
    "\n",
    "# # Convert keywords into strings (if they are lists) and create a TF-IDF matrix for keywords\n",
    "# # Ensure keywords are properly joined into a string for vectorization\n",
    "# keywords_ = [' '.join(keywords) if isinstance(keywords, list) else keywords for keywords in df['keywords']]\n",
    "# vector = CountVectorizer(stop_words='english')\n",
    "# vector_keywords_matrix = vector.fit_transform(keywords_)\n",
    "\n",
    "# # Extract genres and titles as lists for similarity computation\n",
    "# movie_genres = df['genres'].tolist()\n",
    "# movie_titles = df['title'].tolist()\n",
    "# movie_ids = df['movie_id'].tolist()\n",
    "\n",
    "# # Calculate cosine similarity for both overview and keywords\n",
    "# cosine_sim_overview = cosine_similarity(tfidf_overview_matrix)\n",
    "# cosine_sim_keywords = cosine_similarity(vector_keywords_matrix)\n",
    "\n",
    "### MEMOERY ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].fillna('')\n",
    "df['keywords'] = df['keywords'].fillna('')\n",
    "\n",
    "tfidf_overview = TfidfVectorizer(stop_words='english')\n",
    "tfidf_overview_matrix = tfidf_overview.fit_transform(df['overview'])\n",
    "\n",
    "keywords_ = [' '.join(kw) if isinstance(kw, list) else kw for kw in df['keywords']]\n",
    "tfidf_keywords = TfidfVectorizer(stop_words='english')\n",
    "vector_keywords_matrix = tfidf_keywords.fit_transform(keywords_)\n",
    "\n",
    "overview_nn = NearestNeighbors(n_neighbors=100, metric='cosine', algorithm='brute')\n",
    "overview_nn.fit(tfidf_overview_matrix)\n",
    "overview_distances, overview_indices = overview_nn.kneighbors(tfidf_overview_matrix)\n",
    "\n",
    "keyword_nn = NearestNeighbors(n_neighbors=100, metric='cosine', algorithm='brute')\n",
    "keyword_nn.fit(vector_keywords_matrix)\n",
    "keyword_distances, keyword_indices = keyword_nn.kneighbors(vector_keywords_matrix)\n",
    "\n",
    "movie_genres = df['genres'].tolist()\n",
    "movie_titles = df['title'].tolist()\n",
    "movie_ids = df['movie_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96d2532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_from_knn(model, matrix, idx1, idx2):\n",
    "    distances, indices = model.kneighbors(matrix[idx1], n_neighbors=50)\n",
    "    if idx2 in indices[0]:\n",
    "        position = list(indices[0]).index(idx2)\n",
    "        return 1 - distances[0][position]\n",
    "    return 0.0\n",
    "\n",
    "def combined_similarity(movie1_index, movie2_index, genre_weight=0.2, overview_weight=0.5, keyword_weight=0.3):\n",
    "    genre_sim = jaccard_similarity(set(movie_genres[movie1_index]), set(movie_genres[movie2_index]))\n",
    "    overview_sim = get_similarity_from_knn(overview_nn, tfidf_overview_matrix, movie1_index, movie2_index)\n",
    "    keyword_sim = get_similarity_from_knn(keyword_nn, vector_keywords_matrix, movie1_index, movie2_index)\n",
    "    return (genre_weight * genre_sim) + (overview_weight * overview_sim) + (keyword_weight * keyword_sim)\n",
    "\n",
    "def get_recommendations_by_title(movie_title, top_n=10):\n",
    "    if not isinstance(movie_title, str):\n",
    "        return \"Error: Movie title must be a string.\"\n",
    "\n",
    "    matching_indices = [i for i, title in enumerate(movie_titles) if title == movie_title]\n",
    "    if not matching_indices:\n",
    "        return f\"Error: '{movie_title}' not found in the dataset.\"\n",
    "    if len(matching_indices) > 1:\n",
    "        print(f\"⚠️ Warning: The title '{movie_title}' appears {len(matching_indices)} times. Using the first occurrence.\")\n",
    "\n",
    "    movie_index = matching_indices[0]\n",
    "    similarities = [\n",
    "        (i, combined_similarity(movie_index, i))\n",
    "        for i in range(len(movie_titles)) if i != movie_index\n",
    "    ]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nRecommendations for '{movie_title}' (index: {movie_index}):\")\n",
    "    for i, (idx, sim) in enumerate(similarities[:top_n], 1):\n",
    "        print(f\"{i}. Movie: {movie_titles[idx]} (ID: {movie_ids[idx]}), Similarity: {sim:.4f}\")\n",
    "\n",
    "def get_recommendations_by_id(input_id, top_n=10):\n",
    "    try:\n",
    "        movie_id = int(input_id)\n",
    "    except (ValueError, TypeError):\n",
    "        return \"Error: Movie ID must be an integer.\"\n",
    "\n",
    "    try:\n",
    "        movie_index = movie_ids.index(movie_id)\n",
    "    except ValueError:\n",
    "        return f\"Error: Movie ID {movie_id} not found in the dataset.\"\n",
    "\n",
    "    similarities = [\n",
    "        (i, combined_similarity(movie_index, i))\n",
    "        for i in range(len(movie_ids)) if i != movie_index\n",
    "    ]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nRecommendations for Movie ID {movie_id} ({movie_titles[movie_index]}):\")\n",
    "    for i, (idx, sim) in enumerate(similarities[:top_n], 1):\n",
    "        print(f\"{i}. Movie: {movie_titles[idx]} (ID: {movie_ids[idx]}), Similarity: {sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
