[2025-06-18T03:07:23.331+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-06-18T03:07:23.348+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_read_from_bigquery.read_bq manual__2025-06-18T03:07:22.910038+00:00 [queued]>
[2025-06-18T03:07:23.357+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_read_from_bigquery.read_bq manual__2025-06-18T03:07:22.910038+00:00 [queued]>
[2025-06-18T03:07:23.357+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-06-18T03:07:23.372+0000] {taskinstance.py:2890} INFO - Executing <Task(SparkSubmitOperator): read_bq> on 2025-06-18 03:07:22.910038+00:00
[2025-06-18T03:07:23.379+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_read_from_bigquery', 'read_bq', 'manual__2025-06-18T03:07:22.910038+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/test_scipt/test_read_bq.py', '--cfg-path', '/tmp/tmpdq1a0sy9']
[2025-06-18T03:07:23.381+0000] {standard_task_runner.py:105} INFO - Job 6: Subtask read_bq
[2025-06-18T03:07:23.382+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=2889) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-06-18T03:07:23.383+0000] {standard_task_runner.py:72} INFO - Started process 2890 to run task
[2025-06-18T03:07:23.423+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_read_from_bigquery.read_bq manual__2025-06-18T03:07:22.910038+00:00 [running]> on host 9a450f683bb5
[2025-06-18T03:07:23.498+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='you' AIRFLOW_CTX_DAG_ID='spark_read_from_bigquery' AIRFLOW_CTX_TASK_ID='read_bq' AIRFLOW_CTX_EXECUTION_DATE='2025-06-18T03:07:22.910038+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-18T03:07:22.910038+00:00'
[2025-06-18T03:07:23.499+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-06-18T03:07:23.522+0000] {base.py:84} INFO - Retrieving connection 'spark_default'
[2025-06-18T03:07:23.523+0000] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --packages com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.42.2 --name spark_read_bq_job /opt/bitnami/spark/app/read_data_bq.py
[2025-06-18T03:07:24.805+0000] {spark_submit.py:521} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-06-18T03:07:24.873+0000] {spark_submit.py:521} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-06-18T03:07:24.873+0000] {spark_submit.py:521} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-06-18T03:07:24.877+0000] {spark_submit.py:521} INFO - com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency
[2025-06-18T03:07:24.877+0000] {spark_submit.py:521} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-5327b2e5-9627-4e27-91cc-045f824ed121;1.0
[2025-06-18T03:07:24.878+0000] {spark_submit.py:521} INFO - confs: [default]
[2025-06-18T03:07:25.005+0000] {spark_submit.py:521} INFO - found com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.42.2 in central
[2025-06-18T03:07:25.020+0000] {spark_submit.py:521} INFO - :: resolution report :: resolve 137ms :: artifacts dl 5ms
[2025-06-18T03:07:25.021+0000] {spark_submit.py:521} INFO - :: modules in use:
[2025-06-18T03:07:25.021+0000] {spark_submit.py:521} INFO - com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.42.2 from central in [default]
[2025-06-18T03:07:25.022+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-06-18T03:07:25.022+0000] {spark_submit.py:521} INFO - |                  |            modules            ||   artifacts   |
[2025-06-18T03:07:25.023+0000] {spark_submit.py:521} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-06-18T03:07:25.024+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-06-18T03:07:25.024+0000] {spark_submit.py:521} INFO - |      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
[2025-06-18T03:07:25.025+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-06-18T03:07:25.025+0000] {spark_submit.py:521} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-5327b2e5-9627-4e27-91cc-045f824ed121
[2025-06-18T03:07:25.025+0000] {spark_submit.py:521} INFO - confs: [default]
[2025-06-18T03:07:25.029+0000] {spark_submit.py:521} INFO - 0 artifacts copied, 1 already retrieved (0kB/4ms)
[2025-06-18T03:07:25.196+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-06-18T03:07:25.760+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SparkContext: Running Spark version 3.5.2
[2025-06-18T03:07:25.761+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SparkContext: OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64
[2025-06-18T03:07:25.761+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SparkContext: Java version 17.0.15
[2025-06-18T03:07:25.778+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO ResourceUtils: ==============================================================
[2025-06-18T03:07:25.779+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-06-18T03:07:25.779+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO ResourceUtils: ==============================================================
[2025-06-18T03:07:25.780+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SparkContext: Submitted application: ReadFromBigQuery
[2025-06-18T03:07:25.797+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-06-18T03:07:25.803+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO ResourceProfile: Limiting resource is cpu
[2025-06-18T03:07:25.804+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-06-18T03:07:25.841+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SecurityManager: Changing view acls to: ***
[2025-06-18T03:07:25.841+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SecurityManager: Changing modify acls to: ***
[2025-06-18T03:07:25.842+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SecurityManager: Changing view acls groups to:
[2025-06-18T03:07:25.842+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SecurityManager: Changing modify acls groups to:
[2025-06-18T03:07:25.843+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-06-18T03:07:24.272+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO Utils: Successfully started service 'sparkDriver' on port 43981.
[2025-06-18T03:07:24.309+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO SparkEnv: Registering MapOutputTracker
[2025-06-18T03:07:24.337+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO SparkEnv: Registering BlockManagerMaster
[2025-06-18T03:07:24.365+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-06-18T03:07:24.366+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-06-18T03:07:24.369+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-06-18T03:07:24.391+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-452fbf30-f7e0-4cf1-a2c2-96e691a0c46e
[2025-06-18T03:07:24.404+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-06-18T03:07:24.420+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-06-18T03:07:24.533+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-06-18T03:07:24.579+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-06-18T03:07:24.610+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.42.2.jar at spark://9a450f683bb5:43981/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.42.2.jar with timestamp 1750216045753
[2025-06-18T03:07:24.613+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.42.2.jar at spark://9a450f683bb5:43981/files/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.42.2.jar with timestamp 1750216045753
[2025-06-18T03:07:24.615+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO Utils: Copying /home/***/.ivy2/jars/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.42.2.jar to /tmp/spark-2ef0e94c-e5b0-434d-a9be-746a7e09f4b8/userFiles-a175ac5f-5959-4993-a0a4-8d34d1ea7c32/com.google.cloud.spark_spark-bigquery-with-dependencies_2.12-0.42.2.jar
[2025-06-18T03:07:24.726+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-06-18T03:07:24.762+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 21 ms (0 ms spent in bootstraps)
[2025-06-18T03:07:24.835+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250618030724-0009
[2025-06-18T03:07:24.837+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250618030724-0009/0 on worker-20250618015902-172.18.0.2-42529 (172.18.0.2:42529) with 2 core(s)
[2025-06-18T03:07:24.839+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250618030724-0009/0 on hostPort 172.18.0.2:42529 with 2 core(s), 1024.0 MiB RAM
[2025-06-18T03:07:24.844+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40855.
[2025-06-18T03:07:24.844+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO NettyBlockTransferService: Server created on 9a450f683bb5:40855
[2025-06-18T03:07:24.846+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-06-18T03:07:24.853+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9a450f683bb5, 40855, None)
[2025-06-18T03:07:24.856+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManagerMasterEndpoint: Registering block manager 9a450f683bb5:40855 with 434.4 MiB RAM, BlockManagerId(driver, 9a450f683bb5, 40855, None)
[2025-06-18T03:07:24.859+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9a450f683bb5, 40855, None)
[2025-06-18T03:07:24.860+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9a450f683bb5, 40855, None)
[2025-06-18T03:07:24.884+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250618030724-0009/0 is now RUNNING
[2025-06-18T03:07:25.038+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-06-18T03:07:25.215+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-06-18T03:07:25.217+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:25 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-06-18T03:07:26.127+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:26 INFO SparkBigQueryConnectorModule: Registering cleanup jobs listener, should happen just once
[2025-06-18T03:07:27.138+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:27 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43352) with ID 0,  ResourceProfileId 0
[2025-06-18T03:07:27.212+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45649 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.2, 45649, None)
[2025-06-18T03:07:28.847+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:28 INFO DirectBigQueryRelation: |Querying table datapipeline467803.tmdb_dw.fact_movie, parameters sent from Spark:|requiredColumns=[movie_id,vote_average,popularity,vote_count,budget,revenue,runtime],|filter=[]
[2025-06-18T03:07:29.331+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:29 INFO BigQueryClientFactory: Channel pool size set to 1
[2025-06-18T03:07:29.650+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:29 INFO ReadSessionCreator: |creation a read session for table null, parameters: |selectedFields=[movie_id,vote_average,popularity,vote_count,budget,revenue,runtime],|filter=[None]|snapshotTimeMillis[None]
[2025-06-18T03:07:31.246+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO ReadSessionCreator: Read session:{"readSessionName":"projects/datapipeline467803/locations/us/sessions/CAISDHh4SnVpdG5EeUl5YhoCamQaAmpx","readSessionCreationStartTime":"2025-06-18T03:07:28.851525570Z","readSessionCreationEndTime":"2025-06-18T03:07:31.220000832Z","readSessionPrepDuration":816,"readSessionCreationDuration":1551,"readSessionDuration":2368}
[2025-06-18T03:07:31.247+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO ReadSessionCreator: Received 1 partitions from the BigQuery Storage API for session projects/datapipeline467803/locations/us/sessions/CAISDHh4SnVpdG5EeUl5YhoCamQaAmpx. Notice that the number of streams in actual may be lower than the requested number, depending on the amount parallelism that is reasonable for the table and the maximum amount of parallelism allowed by the system.
[2025-06-18T03:07:31.249+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO BigQueryRDDFactory: Created read session for table 'datapipeline467803.tmdb_dw.fact_movie': projects/datapipeline467803/locations/us/sessions/CAISDHh4SnVpdG5EeUl5YhoCamQaAmpx
[2025-06-18T03:07:31.732+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO CodeGenerator: Code generated in 158.811756 ms
[2025-06-18T03:07:31.778+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-06-18T03:07:31.790+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-06-18T03:07:31.791+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2025-06-18T03:07:31.791+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO DAGScheduler: Parents of final stage: List()
[2025-06-18T03:07:31.792+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO DAGScheduler: Missing parents: List()
[2025-06-18T03:07:31.795+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-06-18T03:07:31.878+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 24.9 KiB, free 434.4 MiB)
[2025-06-18T03:07:31.909+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 434.4 MiB)
[2025-06-18T03:07:31.913+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 9a450f683bb5:40855 (size: 11.8 KiB, free: 434.4 MiB)
[2025-06-18T03:07:31.920+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-06-18T03:07:31.937+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-06-18T03:07:31.937+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-06-18T03:07:31.962+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 0, partition 0, PROCESS_LOCAL, 9300 bytes)
[2025-06-18T03:07:32.160+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:45649 (size: 11.8 KiB, free: 434.4 MiB)
[2025-06-18T03:07:34.576+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2623 ms on 172.18.0.2 (executor 0) (1/1)
[2025-06-18T03:07:34.577+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-06-18T03:07:34.581+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:34 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.778 s
[2025-06-18T03:07:34.583+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-06-18T03:07:34.584+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-06-18T03:07:34.585+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:34 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.806786 s
[2025-06-18T03:07:35.355+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 9a450f683bb5:40855 in memory (size: 11.8 KiB, free: 434.4 MiB)
[2025-06-18T03:07:35.363+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:45649 in memory (size: 11.8 KiB, free: 434.4 MiB)
[2025-06-18T03:07:35.587+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO CodeGenerator: Code generated in 21.46973 ms
[2025-06-18T03:07:35.596+0000] {spark_submit.py:521} INFO - +--------+------------+----------+----------+---------+----------+-------+
[2025-06-18T03:07:35.597+0000] {spark_submit.py:521} INFO - |movie_id|vote_average|popularity|vote_count|   budget|   revenue|runtime|
[2025-06-18T03:07:35.597+0000] {spark_submit.py:521} INFO - +--------+------------+----------+----------+---------+----------+-------+
[2025-06-18T03:07:35.598+0000] {spark_submit.py:521} INFO - |       4|         7.8|      82.3|     15000|237000000|2800000000|    162|
[2025-06-18T03:07:35.598+0000] {spark_submit.py:521} INFO - |       1|         8.8|      90.2|     21000|160000000| 829000000|    148|
[2025-06-18T03:07:35.598+0000] {spark_submit.py:521} INFO - |       2|         8.7|      88.5|     18000| 63000000| 465000000|    136|
[2025-06-18T03:07:35.599+0000] {spark_submit.py:521} INFO - |       3|         8.6|      85.0|     19500|165000000| 677000000|    169|
[2025-06-18T03:07:35.599+0000] {spark_submit.py:521} INFO - |       5|         8.5|      75.6|     13000| 15000000| 355000000|    125|
[2025-06-18T03:07:35.599+0000] {spark_submit.py:521} INFO - +--------+------------+----------+----------+---------+----------+-------+
[2025-06-18T03:07:35.600+0000] {spark_submit.py:521} INFO - 
[2025-06-18T03:07:35.600+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-06-18T03:07:35.601+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO SparkBigQueryConnectorModule: In SparkListener.onApplicationEnd, going to activate cleanup jobs
[2025-06-18T03:07:35.601+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BigQueryClient: Running cleanup jobs. Jobs count is 0
[2025-06-18T03:07:35.601+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BigQueryClient: Clearing the cleanup jobs list
[2025-06-18T03:07:35.601+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BigQueryClient: Finished to run cleanup jobs.
[2025-06-18T03:07:35.606+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO SparkUI: Stopped Spark web UI at http://9a450f683bb5:4040
[2025-06-18T03:07:35.610+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-06-18T03:07:35.611+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-06-18T03:07:35.625+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-06-18T03:07:35.640+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO MemoryStore: MemoryStore cleared
[2025-06-18T03:07:35.641+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BlockManager: BlockManager stopped
[2025-06-18T03:07:35.643+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-06-18T03:07:35.645+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-06-18T03:07:35.662+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:35 INFO SparkContext: Successfully stopped SparkContext
[2025-06-18T03:07:36.157+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:36 INFO ShutdownHookManager: Shutdown hook called
[2025-06-18T03:07:36.157+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2cd2866e-f9d6-4066-be7c-d9efebf706a6
[2025-06-18T03:07:36.161+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ef0e94c-e5b0-434d-a9be-746a7e09f4b8
[2025-06-18T03:07:36.165+0000] {spark_submit.py:521} INFO - 25/06/18 03:07:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ef0e94c-e5b0-434d-a9be-746a7e09f4b8/pyspark-a830583d-e7e9-472b-80f7-a9dee1fda813
[2025-06-18T03:07:36.546+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-06-18T03:07:36.547+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_read_from_bigquery, task_id=read_bq, run_id=manual__2025-06-18T03:07:22.910038+00:00, execution_date=20250618T030722, start_date=20250618T030723, end_date=20250618T030736
[2025-06-18T03:07:36.620+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-06-18T03:07:36.639+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-18T03:07:36.641+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
