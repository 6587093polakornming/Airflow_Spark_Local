# TMDB Recommendation Flow

**Author**: Mr. Polakorn Anatapakorn  
**Role**: Data Engineer Intern, Bluebik Vulcan

---

## Description

TMDB Recommendation Flow is a modern data pipeline project designed to automate and streamline movie recommendation dataset generation using **Apache Airflow**, **Apache Spark**, and **Google Cloud Platform (BigQuery)**. The pipeline processes TMDB movie data through several ETL stages‚Äîcleansing, transformation, exporting, and modeling‚Äîto produce ready-to-use data for recommendation systems.

---

## Project Overview

The pipeline is orchestrated using **Airflow DAGs**, processes data using **Apache Spark**, and stores it in **BigQuery**. Output can be used for content-based or collaborative filtering movie recommendation systems.

---

## Table of Contents

- [Project Overview](#project-overview)
- [Project Structure](#project-structure)
- [Architecture Overview](#architecture-overview)
- [Tools & Versions](#tools--versions)
- [Installation](#installation)
- [Usage](#usage)
- [Airflow Connection](#airflow-connection)
- [Credits & Use Case](#credits--use-case)
- [Python Dependencies](#python-dependencies)
- [License](#license)

---

## Project Structure

```
TMDB_RecoFlow/
‚îú‚îÄ‚îÄ airflow/                 # Contains Airflow DAGs and utility scripts
‚îú‚îÄ‚îÄ spark/                   # Spark ETL scripts for preprocessing and transformation
‚îú‚îÄ‚îÄ eda/                     # Jupyter notebooks for data exploration and modeling
‚îú‚îÄ‚îÄ model/                   # Modeling scripts for recommendations
‚îú‚îÄ‚îÄ Dockerfile               # Dockerfile for Airflow and Spark services
‚îú‚îÄ‚îÄ Dockerfile.jupyter       # Dockerfile for JupyterLab with PySpark
‚îú‚îÄ‚îÄ docker-compose.yml       # Orchestration of all services
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
```

---

## Architecture Overview
![Architecture Overview Figure](doc/diagram/TMDB_arch_ov.drawio.png)

---

## Tools & Versions

- ![Airflow](https://img.shields.io/badge/Airflow-2.10.5-blue?logo=apache-airflow&logoColor=white)  
  via provider: `apache-airflow-providers-apache-spark==4.3.0`

- ![Spark](https://img.shields.io/badge/Apache_Spark-3.5.2-FD7F20?logo=apache-spark&logoColor=white)

- ![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python&logoColor=white)

- ![Hadoop](https://img.shields.io/badge/Hadoop-3.0-yellow?logo=apache-hadoop&logoColor=black)

- ![Docker Compose](https://img.shields.io/badge/Docker--Compose-blue?logo=docker&logoColor=white)

- ![BigQuery](https://img.shields.io/badge/BigQuery-GCP-blue?logo=google-cloud&logoColor=white)  
  ![GCS](https://img.shields.io/badge/Cloud_Storage-GCP-blue?logo=google-cloud&logoColor=white)

---

## Installation

### Prerequisites
- Docker & Docker Compose
- GCP service account credentials (for BigQuery & GCS)

### Steps

```bash
# Clone the repository
git clone https://github.com/6587093polakornming/TMDB_RecoFlow.git
cd TMDB_RecoFlow

# Copy and configure your environment variables
cp example.env .env

# Build and start the services
docker-compose up --build
```

Ensure your GCP credentials and environment variables (e.g., project ID, bucket name) are correctly configured in `.env`.

---

## Usage

1. **Airflow UI**:
   - Access at `http://localhost:8080`
   - Trigger DAGs manually or set a schedule

2. **Workflow Path**:
   - Airflow DAGs: `airflow/dags/`
   - Spark Scripts: `spark/app/`
   - Notebooks: `eda/`

3. **ETL Process**:
   - Ingest and validate movie data
   - Clean and transform using Spark
   - Export to BigQuery
   - Run model to generate recommendations

---

## Airflow Connection
Airflow uses the following external service connections:

### üß† Spark Connection
- Airflow uses `SparkSubmitOperator` to submit Spark jobs.
- Connection string:
  ```bash
  spark://spark-master:7077
  ```
- This is defined as `spark_default` in `docker-compose.yml`.

### ‚òÅÔ∏è Google Cloud Connection
- Airflow interacts with BigQuery and GCS using the connection ID:
  ```bash
  google_cloud_default
  ```
- This connection must be configured in the Airflow UI or via environment variables.
- The service account must have permission to:
  - Access BigQuery project: `YOUR-GCP-PROJECT-ID`
  - Read/write from GCS bucket: `YOUR-GCS-BUCKET-NAME`
- Ensure the service account key is available in your container and properly mounted..

---

## Credits & Use Case

- **Dataset**: [TMDB Movies Dataset - Daily Updates](https://www.kaggle.com/code/asaniczka/tmdb-movies-daily-updates)
- **Use Case Reference**: [Recommendation System by moridata](https://www.kaggle.com/code/moridata/recommendation-system-movie-recommendation)

---

## Python Dependencies

Install locally (if needed outside Docker):

```bash
pip install --no-cache-dir \
  "apache-airflow-providers-apache-spark==4.3.0" \
  "apache-airflow-providers-google" \
  "google-cloud-bigquery" \
  "pyarrow" \
  "fastparquet"
```

---

## License

This project is licensed under the **MIT License**. See the `LICENSE` file for more details.
